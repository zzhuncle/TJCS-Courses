{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.feature import daisy\n",
    "import numpy as np\n",
    "import joblib\n",
    "# To read image file and save image feature descriptions\n",
    "import os\n",
    "import time\n",
    "# import glob\n",
    "import pickle as pk\n",
    "from config import *\n",
    "import operator\n",
    "from functools import reduce\n",
    "from sklearn.svm import LinearSVC\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpickle(file):\n",
    "    import pickle\n",
    "    with open(file, 'rb') as fo:\n",
    "        dict = pickle.load(fo, encoding='bytes')\n",
    "    return dict\n",
    "\n",
    "def getData(filePath):\n",
    "    TrainData = []\n",
    "    flag_train=0\n",
    "    flag_test=0\n",
    "\n",
    "\n",
    "    for childDir in os.listdir(filePath):\n",
    "        if 'data_batch' in childDir:\n",
    "            f = os.path.join(filePath, childDir)\n",
    "            data = unpickle(f)\n",
    "            # train = np.reshape(data[str.encode('data')], (10000, 3, 32 * 32))\n",
    "            # If your python version do not support to use this way to transport str to bytes.\n",
    "            # Think another way and you can.\n",
    "            train = np.reshape(data[b'data'], (10000, 3, 32 * 32))\n",
    "            if(flag_train==0):\n",
    "                train_label=(np.reshape(data[b'labels'], (10000, 1)))\n",
    "                flag_train=1\n",
    "            else:\n",
    "                train_label=np.concatenate((train_label,np.reshape(data[b'labels'], (10000, 1))))\n",
    "\n",
    "            datalebels = zip(train)\n",
    "            TrainData.extend(datalebels)\n",
    "\n",
    "\n",
    "        cnt=0\n",
    "        if childDir == \"test_batch\":\n",
    "            f = os.path.join(filePath, childDir)\n",
    "            data = unpickle(f)\n",
    "            test = np.reshape(data[b'data'], (10000, 3, 32 * 32))\n",
    "            if (flag_test == 0):\n",
    "                test_label = (np.reshape(data[b'labels'], (10000, 1)))\n",
    "                flag_test = 1\n",
    "            else:\n",
    "                test_label = np.concatenate((test_label, np.reshape(data[b'labels'], (10000, 1))))\n",
    "            TestData = zip(test)\n",
    "\n",
    "    train_label=train_label.ravel()\n",
    "    test_label=test_label.ravel()\n",
    "    return TrainData, TestData,train_label,test_label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# get Daisy Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getFeat(TrainData, TestData):\n",
    "\n",
    "    #这两个数组，注意，列的个数和HOG参数选择有关，不知道怎么自适应所以不调了\n",
    "    train_feature=np.zeros(shape=(50000,200))\n",
    "    test_feature=np.zeros(shape=(10000,200))\n",
    "    cnt=0\n",
    "    for data in TestData:\n",
    "        image = np.reshape(data[0].T, (32, 32, 3))\n",
    "        gray = rgb2gray(image)/255.0\n",
    "        fd = daisy(gray)\n",
    "\n",
    "        test_feature[cnt]=fd\n",
    "        cnt=cnt+1\n",
    "\n",
    "    print(\"Test features are extracted and saved.\")\n",
    "    cnt=0\n",
    "    for data in TrainData:\n",
    "        image = np.reshape(data[0].T, (32, 32, 3))\n",
    "        gray = rgb2gray(image)/255.0\n",
    "        fd = daisy(gray)\n",
    "\n",
    "        train_feature[cnt] = fd\n",
    "        cnt = cnt + 1\n",
    "\n",
    "    print(\"Train features are extracted and saved.\")\n",
    "\n",
    "    return train_feature,test_feature\n",
    "\n",
    "\n",
    "def rgb2gray(im):\n",
    "    gray = im[:, :, 0]*0.2989+im[:, :, 1]*0.5870+im[:, :, 2]*0.1140\n",
    "    return gray"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test features are extracted and saved.\n",
      "Train features are extracted and saved.\n",
      "Features are extracted\n"
     ]
    }
   ],
   "source": [
    "filePath = r'C:\\Users\\包广垠\\PythonWorkPlace\\ML-task2\\cifar-10-batches-py'\n",
    "TrainData, TestData, y_train, y_test = getData(filePath)\n",
    "x_train_hog,x_test_hog = getFeat(TrainData, TestData)\n",
    "print(\"Features are extracted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from sklearn import svm\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn import tree\n",
    "from sklearn import naive_bayes\n",
    "\n",
    "import xgboost\n",
    "from sklearn import ensemble"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction using Bernoulli Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BernoulliNB()"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Training\n",
    "bbn = naive_bayes.BernoulliNB()\n",
    "bbn.fit(x_train_hog, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_bbn = bbn.predict(x_test_hog)\n",
    "bernoulli_naive_bayes_score = accuracy_score(y_test, y_pred_bbn)\n",
    "bernoulli_naive_bayes_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction using Gaussian Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GaussianNB()"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Training\n",
    "gbn = naive_bayes.GaussianNB()\n",
    "gbn.fit(x_train_hog, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3839"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Predicting\n",
    "y_pred_gbn = gbn.predict(x_test_hog)\n",
    "gaussian_naive_bayes_score = accuracy_score(y_test, y_pred_gbn)\n",
    "gaussian_naive_bayes_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction using Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier()"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Training\n",
    "rf = RandomForestClassifier()\n",
    "rf.fit(x_train_hog, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5228"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Predicting\n",
    "y_pred_rf = rf.predict(x_test_hog)\n",
    "random_forest_score = accuracy_score(y_test, y_pred_rf)\n",
    "random_forest_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction using KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier()"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Training \n",
    "knn = KNeighborsClassifier()\n",
    "knn.fit(x_train_hog, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4859"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Predicting\n",
    "y_pred_knn = knn.predict(x_test_hog)\n",
    "knn_score = accuracy_score(y_test, y_pred_knn)\n",
    "knn_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction using Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Training \n",
    "lr = LogisticRegression()\n",
    "lr.fit(x_train_hog, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3604"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Predicting\n",
    "y_pred_lr = lr.predict(x_test_hog)\n",
    "logistic_regression_score = accuracy_score(y_test, y_pred_lr)\n",
    "logistic_regression_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction using Linear SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearSVC()"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Training\n",
    "lsvc = svm.LinearSVC()\n",
    "lsvc.fit(x_train_hog, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4117"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Predicting \n",
    "y_pred_lsvm = lsvc.predict(x_test_hog)\n",
    "linear_svc_score = accuracy_score(y_test, y_pred_lsvm)\n",
    "linear_svc_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction using SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC()"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Training\n",
    "svc = svm.SVC()\n",
    "svc.fit(x_train_hog, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6427"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Predicting \n",
    "y_pred_svm = svc.predict(x_test_hog)\n",
    "svc_score = accuracy_score(y_test, y_pred_svm)\n",
    "svc_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## prediction using Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier()"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Training\n",
    "dt = tree.DecisionTreeClassifier()\n",
    "dt.fit(x_train_hog, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3145"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Predicting\n",
    "y_pred_dt = dt.predict(x_test_hog)\n",
    "decision_tree_score = accuracy_score(y_test, y_pred_dt)\n",
    "decision_tree_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## prediction using XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\包广垠\\AppData\\Roaming\\Python\\Python38\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23:41:53] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
       "              importance_type='gain', interaction_constraints='',\n",
       "              learning_rate=0.300000012, max_delta_step=0, max_depth=6,\n",
       "              min_child_weight=1, missing=nan, monotone_constraints='()',\n",
       "              n_estimators=100, n_jobs=8, num_parallel_tree=1,\n",
       "              objective='multi:softprob', random_state=0, reg_alpha=0,\n",
       "              reg_lambda=1, scale_pos_weight=None, subsample=1,\n",
       "              tree_method='exact', validate_parameters=1, verbosity=None)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Training\n",
    "xgb = xgboost.XGBClassifier()\n",
    "xgb.fit(x_train_hog, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6063"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Predicting\n",
    "y_pred_xgb = xgb.predict(x_test_hog)\n",
    "XGBoost_score = accuracy_score(y_test, y_pred_xgb)\n",
    "XGBoost_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## prediction using AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AdaBoostClassifier()"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Training\n",
    "adb = ensemble.AdaBoostClassifier()\n",
    "adb.fit(x_train_hog, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3648"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Predicting\n",
    "y_pred_adb = adb.predict(x_test_hog)\n",
    "AdaBoost_score = accuracy_score(y_test, y_pred_adb)\n",
    "AdaBoost_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison between various Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bernoulli Naive Bayes :  0.1\n",
      "Gaussian Naive Bayes :  0.3839\n",
      "Random Forest Classifier :  0.5228\n",
      "K Nearest Neighbors :  0.4859\n",
      "Logistic Regression :  0.3604\n",
      "Linear Support Vector Classifier :  0.4117\n",
      "Support Vector Classifier :  0.6427\n",
      "Decision Tree Classifier :  0.3145\n",
      "XGBoost :  0.6063\n",
      "AdaBoost :  0.3648\n"
     ]
    }
   ],
   "source": [
    "print(\"Bernoulli Naive Bayes : \", bernoulli_naive_bayes_score)\n",
    "print(\"Gaussian Naive Bayes : \", gaussian_naive_bayes_score)\n",
    "print(\"Random Forest Classifier : \", random_forest_score)\n",
    "print(\"K Nearest Neighbors : \", knn_score)\n",
    "print(\"Logistic Regression : \", logistic_regression_score)\n",
    "print(\"Linear Support Vector Classifier : \", linear_svc_score)\n",
    "print(\"Support Vector Classifier : \", svc_score)\n",
    "print(\"Decision Tree Classifier : \", decision_tree_score)\n",
    "print(\"XGBoost : \", XGBoost_score)\n",
    "print(\"AdaBoost : \", AdaBoost_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
